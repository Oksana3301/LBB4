---
title: "you got the heart sick?"
author: "Atika Dewi Suryani"
date: "`r format(Sys.Date(), '%B %e, %Y')`"
output:
  html_document:
   toc: true
   toc_float: 
    collapsed: false
   number_sections: true
   toc_depth: 3
   theme: flatly
   highlight: breezedark
   df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", warning = F, message = F)
```

<style>
body {
text-align: justify}
</style>

Prediction of heart disease patients in a hospital will predict whether they are sick or not based on the category of several supporting variables

```{r}
knitr::include_graphics("image/heart.jfif")
```
# Setup {.tabset .tabset pils}
```{r}
library(dplyr)
library(gtools)
library(gmodels)
library(ggplot2)
library(class)
library(tidyr)
library(caret)
```

# Load Data 

Data that we are using is based from the Kaggle. You can direct download direct from Kaggle.
```{r}
heart <- read.csv("data/heart.csv")
str(heart)
```
**Variabel Info**

- **age** : old of patients

- **sex** : gender (1 : male, 0 : female)

- **cp** : severity

- **trestbps** : check blood pressure in (mm Hg) when in a hospital

- **chol** : cholesterol

- **fbs** : sugar in blood when fasting>120mg/dl (1:correct, 0: wrong)

- **restecg** : return electrocardiograph

- **thalach** : heart pulse maximum

- **exang** : exercise indunged angine (1 : yes, 0 : no)

- **oldpeak** : ST depression that cause by sport realative when at ease

- **slope** : slope og segmen when at maximum exercise

- **ca** : total number of main vena (0-3) mark by fluoroschopy

- **thal** : (3:normal, 6: permanent disability, 7 : unchange disability)

- **target** : (1 : sick, 0 : not sick)


This is the summary of data that we are using
```{r}
head(heart)
```

# Data Manipulation {.tabset}

In several variables that we used, there are some
unsuitable data type, therefore we need to make some adjustment for some variables.

```{r}
heart <- heart %>% 
  mutate_if(is.integer, as.factor) %>% 
  mutate(sex = factor(sex, levels = c(0,1), 
                      labels = c("Female", 
                                 "Male")),
         fbs =factor(fbs, levels = c(0,1), 
                     labels = c("False", "True")),
         exang = factor(exang, levels = c(0,1), 
                        labels = c("No", "Yes")),
         target = factor(target, levels = c(0,1), 
                        labels = c("Health", "Not Health")))

glimpse(heart)
```
Next, we check if there are missing values 

```{r}
anyNA(heart)
```

# Preprocessing Data

Before we go to make the model, we need to check the proportion and variable target of each variable that we have in ` target` colomn

```{r}
prop.table(table(heart$target))
```

```{r}
table(heart$target)
```

The proportion is `quite balance`.

## Splitting Train-Test

Next step, is we are going to do splitting train-test data. The purpose is in train data that we are going to use for the modelling. Then for the test data we are going to treat to check the model performance to face the unseen data.

```{r}
set.seed(303)
intrain <- sample(nrow(heart), nrow(heart)*0.7)
heart_train <- heart[intrain,]
heart_test <- heart[-intrain,]
heart$target %>% 
  levels()
```

## Modelling

We are gonna using function `glm()` wheren the `target` varible is the response

```{r}
model <- glm(formula =
             target~sex+cp+fbs+exang+oldpeak+slope+
             ca+thal, 
             family = "binomial", 
             data = heart_train)

summary(model)
```

## Model Fitting

From the first model, we still have a lot of predictor variable that are not significant to the variable target, therefore we will try model fitting using `stepwise` method

```{r}
library(MASS)
model2 <- stepAIC(model, direction = "backward")
```

Based in method `backward` in stepwise, we get the model as below : 
```{r}
summary(model2)
```

## Prediction

Using `model2` from `stepwise` method, we try to predict the test data that we already have

```{r}
heart_test$prob_heart<-predict(model2, type = "response", newdata = heart_test)
```

Then we check the probability proportion in prediction data
```{r}
ggplot(heart_test, aes(x=prob_heart)) +
        geom_density(lwd=0.5) +
        labs(title = "Distribution of Probability 
        Prediction Data") +
        
  theme_minimal()
```

From this graphic, shows that the prediction is more into `Not Health` result


```{r}
heart_test$pred_heart <- factor(ifelse(heart_test$prob_heart > 0.5, "Not Health","Health"))

heart_test[1:10, c("pred_heart", "target")]
```
**From syntax below**, when probability `test data > 0.5`, means `Not Health`.

# Model Evaluation

We are going to use `confusion matrix`

```{r}
log_conf <- confusionMatrix(heart_test$pred_heart, heart_test$target, positive = "Not Health")

log_conf
```
```{r}
Recall <- round((27)/(27+8),2)
Specificity <- round((49)/(7+49),2)
Accuracy <- round((27+49)/(27+7+8+49),2)
Precision <- round((49)/(49+8),2)

performance <- cbind.data.frame(Accuracy, Recall, Precision, Specificity)
performance
```

The `model's ability to predict Y targets (Health and Not Health)` was `83.5%`. 
Meanwhile, from the `total actual data of people who are Not Health`, the model was able to guess correctly by `87.5%`. From the `overall actual data on people who were Health`, the model was able to guess correctly by `77.1%`. From the `overall prediction results that can be guessed by the model`, the model is able to correctly `guess the positive class` by `86%`.

# Tuning Cutoff

Known for max threshold from what we are seeing 
```{r}
# tuning cutoff
performa <- function(cutoff, prob, ref, postarget, negtarget) 
{
  predict <- factor(ifelse(prob >= cutoff, postarget, negtarget))
  conf <- caret::confusionMatrix(predict , ref, positive = postarget)
  acc <- conf$overall[1]
  rec <- conf$byClass[1]
  prec <- conf$byClass[3]
  spec <- conf$byClass[2]
  mat <- t(as.matrix(c(rec , acc , prec, spec))) 
  colnames(mat) <- c("recall", "accuracy", "precicion", "specificity")
  return(mat)
}

co <- seq(0.01,0.80,length=100)
result <- matrix(0,100,4)

for(i in 1:100){
  result[i,] = performa(cutoff = co[i], 
                     prob = heart_test$prob_heart, 
                     ref = heart_test$target, 
                     postarget = "Not Health", 
                     negtarget = "Health")
}

data_frame("Recall" = result[,1],
           "Accuracy" = result[,2],
           "Precision" = result[,3],
           "Specificity" = result[,4],
                   "Cutoff" = co) %>% 
  gather(key = "performa", value = "value", 1:4) %>% 
  ggplot(aes(x = Cutoff, y = value, col = performa)) +
  geom_line(lwd = 1.5) +
  scale_color_manual(values = c("darkred","darkgreen","orange", "blue")) +
  scale_y_continuous(breaks = seq(0,1,0.1), limits = c(0,1)) +
  scale_x_continuous(breaks = seq(0,1,0.1)) +
  labs(title = "Tradeoff model perfomance") +
  theme_minimal() +
  theme(legend.position = "top",
        panel.grid.minor.y = element_blank(),
        panel.grid.minor.x = element_blank())
```
By `cutoff 0.5` we have score for `re-call and precision a little bit higher`, but `accuracy and specifity bit lower`.

# Model Interpretation
```{r}
# Odds ratio all coefficients
exp(model2$coefficients) %>% 
  data.frame() 
```
**Model interpretation:** 
`Odds of male sex = 0.206 < 1`
means that people with `male sex are 20.6% smaller for Not Health than women`.

# K-Nearest Neighbor {.tabset}

## Pre Processing Data
Make a dummy variable from category datas that we are using in the classification
```{r}
dmy <- dummyVars(" ~target+sex+cp+fbs+exang+oldpeak+slope+ca+thal", data = heart)
dmy <- data.frame(predict(dmy, newdata = heart))
str(dmy)
```
Delete variable dummy which before only have `2 variables`.

```{r}
dmy$target.Health <- NULL
dmy$sex.Female <- NULL
dmy$fbs.False <- NULL
dmy$exang.No <- NULL
```

Dummy variables labels 
```{r}
names(dmy)
```
Create `data train` and `data test` from dummy data 
```{r}
set.seed(300)
dmy_train <- dmy[intrain,2:21]
dmy_test <- dmy[-intrain,2:21]

dmy_train_label <- dmy[intrain,1]
dmy_test_label <- dmy[-intrain,1]
```

**Predict using KNN**
```{r}
pred_knn <- class::knn(train = dmy_train,
                       test = dmy_test, 
                       cl = dmy_train_label, 
                       k = 17)
```

**Confusion Matrix from KNN Prediction**
```{r}
pred_knn_conf <- confusionMatrix(as.factor(pred_knn), as.factor(dmy_test_label),"1")

pred_knn_conf
```
**Based on the results of the confusion matrix above**, 
we can see that the `model's ability to guess target Y` is `87.9%`. Meanwhile, based on `actual data on people who have Not Health status`, the `model can guess correctly by 94.6%`. based on `actual data of people who have Health status`, the model can guess correctly by `77.14%`. From the `overall prediction results that can be guessed by the model`, the model is able to `correctly guess the positive class of 86.9%`.

# Model Evaluation Logistic Regression and K-NN
```{r}
eval_logit <- data_frame(Accuracy = log_conf$overall[1],
           Recall = log_conf$byClass[1],
           Specificity = log_conf$byClass[2],
           Precision = log_conf$byClass[3])

eval_knn <- data_frame(Accuracy = pred_knn_conf$overall[1],
           Recall = pred_knn_conf$byClass[1],
           Specificity = log_conf$byClass[2],
           Precision = pred_knn_conf$byClass[3])
```

```{r}
# Model Evaluation Logit
eval_logit
```

```{r}
# Model Evaluation K-NN
eval_knn
```

When viewed from the two methods, `namely by using Logistic Regression and K-NN`, the `ability of the model to correctly predict from the actual data of people with Not Health` is `better by using the K-NN` method because it has a value of `precision = 86.9% greater than using the logistic regression method`.

# Conclusion

Personally the model is depend by whom it use, if it use by the doctor, they will really look at the metric precision, where they don't want my model to be wrong in predicting which patients are truly heart sick or who are not.



